{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_columns(df, col):\n",
    "    '''\n",
    "    using a pre-made function \n",
    "    returns a list of the tokenized and stripped of stopwords \n",
    "    '''\n",
    "    text = ' '.join(df[col])\n",
    "    tokens = word_tokenize(text)\n",
    "    # converts the tokens to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    words = [word for word in stripped if word.isalnum()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    if col == 'notes':\n",
    "        #TODO: add another set of stopwords for the notes\n",
    "        remove_words = {'final', 'quarterfinal', 'game', 'jeopardy!', 'semifinal', 'round', 'tournament', 'week', 'reunion', 'ultimate', 'night', 'jeopardy', 'night', 'games'}\n",
    "        stopwords_set = (set(stopwords.words('english'))).union(remove_words)\n",
    "    else:\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stopwords_set]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_q_and_a_col(df):\n",
    "    \"\"\"\n",
    "    Makes a column that concatenates the strings\n",
    "    from the question and answer columns\n",
    "\n",
    "    Args:\n",
    "        df (Pandas DataFrame): \n",
    "    Returns:\n",
    "        Pandas DataFrame with an additional column\n",
    "    \"\"\"    \n",
    "    df['question_and_answer'] = df[\"question\"] + ' ' + df['answer']\n",
    "    return df\n",
    "\n",
    "def make_q_difficulty_col(df):\n",
    "    conditions = [((df['value']<=600) & (df['daily_double']=='no')), #easy\n",
    "                ((df['daily_double']=='no') & ((df['value']==800) | (df['value']==1200))), #average\n",
    "                ((df['daily_double']== 'yes') & (df['round'] == 1)), #average\n",
    "                ((df['daily_double']=='no') & ((df['value']==1000) | (df['value']>=1600))), #hard\n",
    "                ((df['daily_double']== 'yes') & (df['round'] == 2)), #hard\n",
    "                (df['round'] == 3)] # final jeopardy, hard \n",
    "\n",
    "    difficulties = ['easy', 'average', 'average', 'hard', 'hard', 'hard']\n",
    "\n",
    "    df['question_difficulty'] = np.select(conditions, difficulties)\n",
    "    return df\n",
    "\n",
    "#TODO: write docstring\n",
    "def update_df_columns(df):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "    df_new = make_q_and_a_col(df)\n",
    "    df_new = make_q_difficulty_col(df_new)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(filepath):\n",
    "    \"\"\"Reads in a tsv file\n",
    "\n",
    "    Args:\n",
    "        filepath (string): filepath and file name of the \n",
    "            tsv file to be read into as a pandas dataframe\n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"    \n",
    "    return pd.read_csv(filepath, sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: write docstring\n",
    "def make_sub_df(df, fraction = .05, state = 123):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "        fraction (float, optional): [description]. Defaults to .05.\n",
    "        state (int, optional): [description]. Defaults to 123.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    return df.sample(frac = fraction, axis = 0, random_state = state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_df = read_tsv('../data/master_season1-35.tsv')\n",
    "jeopardy_df = update_df_columns(jeopardy_df)\n",
    "regular_episodes = jeopardy_df[jeopardy_df['notes']=='-']\n",
    "special_tournaments = jeopardy_df.drop(regular_episodes.index)\n",
    "\n",
    "regular_episode_sub = make_sub_df(regular_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit a countvectorizer over the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transofrm the x_train, transofrm the x_test\n",
    "#adjust the hyper parameters \n",
    "count_vect = CountVectorizer(ngram_range = (1, 1), \n",
    "                            lowercase=True,  tokenizer=None, \n",
    "                            stop_words='english', analyzer='word',  \n",
    "                              max_features=None)\n",
    "\n",
    "#WHAT DO I PUT IN THE FIT_TRANSFORM\n",
    "x = count_vect.fit_transform(X_train_sample)\n",
    "# x.toarray() or x.todense ??\n",
    "features = count_vect.get_feature_names()\n",
    "# count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can do the same as above with the categories \n",
    "\n",
    "#TODO: do it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster(df, col, n):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "        n ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "    #adjust the hyperparameters\n",
    "    count_vect = CountVectorizer(ngram_range = (1, 1), \n",
    "                            lowercase=True,  tokenizer=None, \n",
    "                            stop_words='english', analyzer='word',  \n",
    "                            max_features=None)\n",
    "\n",
    "    x = count_vect.fit_transform(df[col])\n",
    "    # features = count_vect.get_feature_names()\n",
    "    kmeans = KMeans(n_clusters = 10, random_state = 123).fit(x)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    top_n = np.argsort(centroids)[:, :-n+1:-1]\n",
    "    names = count_vect.get_feature_names()\n",
    "\n",
    "    name_arr = np.array(names)\n",
    "    return f'n = {n}', name_arr[top_n]\n",
    "\n",
    "# kmeans_cluster(regular_episode_sub, 'question_and_answer', 10)\n",
    "#This did kind of a terrible job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT STEPS 1\n",
    "- Look at the `modeling with nmf` and case study code\n",
    "- use a tdidf transformer \n",
    "- pass it into an NMF to get out soft clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 100\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "      <th>question_and_answer</th>\n",
       "      <th>question_difficulty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WHAT'S WRONG WITH YOU?</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>Underactivity of this butterfly-shaped gland s...</td>\n",
       "      <td>Thyroid gland</td>\n",
       "      <td>2001-02-23</td>\n",
       "      <td>-</td>\n",
       "      <td>Thyroid gland Underactivity of this butterfly-...</td>\n",
       "      <td>average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOMETRY</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>(Cheryl of the Clue Crew in front of a blackbo...</td>\n",
       "      <td>eccentric</td>\n",
       "      <td>2003-10-03</td>\n",
       "      <td>-</td>\n",
       "      <td>eccentric (Cheryl of the Clue Crew in front of...</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIBLE PEOPLE MAGAZINE</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>(Alex: That sounds like fun!)</td>\n",
       "      <td>Star Tracks says that scholars believe these s...</td>\n",
       "      <td>Magi/three wise men</td>\n",
       "      <td>2003-06-24</td>\n",
       "      <td>-</td>\n",
       "      <td>Magi/three wise men Star Tracks says that scho...</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ODE TO ENGLAND</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>Headquarters was needed for Peel's new police ...</td>\n",
       "      <td>Scotland Yard</td>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>-</td>\n",
       "      <td>Scotland Yard Headquarters was needed for Peel...</td>\n",
       "      <td>average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\\"C\" IN SCIENCE</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>In order to duplicate itself in cell division,...</td>\n",
       "      <td>chromosomes</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>chromosomes In order to duplicate itself in ce...</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BY THE SEASHORE</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>The dried skeleton of this invertebrate resemb...</td>\n",
       "      <td>Sand Dollar</td>\n",
       "      <td>1996-12-04</td>\n",
       "      <td>-</td>\n",
       "      <td>Sand Dollar The dried skeleton of this inverte...</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPERA</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>Euridice is a soprano role in Monteverdi's \"Th...</td>\n",
       "      <td>Orpheus</td>\n",
       "      <td>1997-07-14</td>\n",
       "      <td>-</td>\n",
       "      <td>Orpheus Euridice is a soprano role in Montever...</td>\n",
       "      <td>average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FURNITURE</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>A long seat designed for 2 or more people, it ...</td>\n",
       "      <td>settee</td>\n",
       "      <td>1989-10-12</td>\n",
       "      <td>-</td>\n",
       "      <td>settee A long seat designed for 2 or more peop...</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RINGING THE OPENING BELL AT THE NYSE</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>On Oct. 11, 2007 this chairman of the Virgin G...</td>\n",
       "      <td>(Richard) Branson</td>\n",
       "      <td>2008-07-22</td>\n",
       "      <td>-</td>\n",
       "      <td>(Richard) Branson On Oct. 11, 2007 this chairm...</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HALLS OF FAME</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>In 1997 their career was \"Stayin' Alive\" with ...</td>\n",
       "      <td>the Bee Gees</td>\n",
       "      <td>1997-10-30</td>\n",
       "      <td>-</td>\n",
       "      <td>the Bee Gees In 1997 their career was \"Stayin'...</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13936 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      round  value daily_double  \\\n",
       "category                                                          \n",
       "WHAT'S WRONG WITH YOU?                    2    800           no   \n",
       "GEOMETRY                                  2   2000           no   \n",
       "BIBLE PEOPLE MAGAZINE                     1    400           no   \n",
       "ODE TO ENGLAND                            1    800           no   \n",
       "\\\"C\" IN SCIENCE                           2   2000           no   \n",
       "...                                     ...    ...          ...   \n",
       "BY THE SEASHORE                           1    400           no   \n",
       "OPERA                                     2    800           no   \n",
       "FURNITURE                                 1    300           no   \n",
       "RINGING THE OPENING BELL AT THE NYSE      1    400           no   \n",
       "HALLS OF FAME                             1    100           no   \n",
       "\n",
       "                                                           comments  \\\n",
       "category                                                              \n",
       "WHAT'S WRONG WITH YOU?                                            -   \n",
       "GEOMETRY                                                          -   \n",
       "BIBLE PEOPLE MAGAZINE                 (Alex: That sounds like fun!)   \n",
       "ODE TO ENGLAND                                                    -   \n",
       "\\\"C\" IN SCIENCE                                                   -   \n",
       "...                                                             ...   \n",
       "BY THE SEASHORE                                                   -   \n",
       "OPERA                                                             -   \n",
       "FURNITURE                                                         -   \n",
       "RINGING THE OPENING BELL AT THE NYSE                              -   \n",
       "HALLS OF FAME                                                     -   \n",
       "\n",
       "                                                                                 answer  \\\n",
       "category                                                                                  \n",
       "WHAT'S WRONG WITH YOU?                Underactivity of this butterfly-shaped gland s...   \n",
       "GEOMETRY                              (Cheryl of the Clue Crew in front of a blackbo...   \n",
       "BIBLE PEOPLE MAGAZINE                 Star Tracks says that scholars believe these s...   \n",
       "ODE TO ENGLAND                        Headquarters was needed for Peel's new police ...   \n",
       "\\\"C\" IN SCIENCE                       In order to duplicate itself in cell division,...   \n",
       "...                                                                                 ...   \n",
       "BY THE SEASHORE                       The dried skeleton of this invertebrate resemb...   \n",
       "OPERA                                 Euridice is a soprano role in Monteverdi's \"Th...   \n",
       "FURNITURE                             A long seat designed for 2 or more people, it ...   \n",
       "RINGING THE OPENING BELL AT THE NYSE  On Oct. 11, 2007 this chairman of the Virgin G...   \n",
       "HALLS OF FAME                         In 1997 their career was \"Stayin' Alive\" with ...   \n",
       "\n",
       "                                                 question    air_date notes  \\\n",
       "category                                                                      \n",
       "WHAT'S WRONG WITH YOU?                      Thyroid gland  2001-02-23     -   \n",
       "GEOMETRY                                        eccentric  2003-10-03     -   \n",
       "BIBLE PEOPLE MAGAZINE                 Magi/three wise men  2003-06-24     -   \n",
       "ODE TO ENGLAND                              Scotland Yard  2003-04-01     -   \n",
       "\\\"C\" IN SCIENCE                               chromosomes  2003-09-10     -   \n",
       "...                                                   ...         ...   ...   \n",
       "BY THE SEASHORE                               Sand Dollar  1996-12-04     -   \n",
       "OPERA                                             Orpheus  1997-07-14     -   \n",
       "FURNITURE                                          settee  1989-10-12     -   \n",
       "RINGING THE OPENING BELL AT THE NYSE    (Richard) Branson  2008-07-22     -   \n",
       "HALLS OF FAME                                the Bee Gees  1997-10-30     -   \n",
       "\n",
       "                                                                    question_and_answer  \\\n",
       "category                                                                                  \n",
       "WHAT'S WRONG WITH YOU?                Thyroid gland Underactivity of this butterfly-...   \n",
       "GEOMETRY                              eccentric (Cheryl of the Clue Crew in front of...   \n",
       "BIBLE PEOPLE MAGAZINE                 Magi/three wise men Star Tracks says that scho...   \n",
       "ODE TO ENGLAND                        Scotland Yard Headquarters was needed for Peel...   \n",
       "\\\"C\" IN SCIENCE                       chromosomes In order to duplicate itself in ce...   \n",
       "...                                                                                 ...   \n",
       "BY THE SEASHORE                       Sand Dollar The dried skeleton of this inverte...   \n",
       "OPERA                                 Orpheus Euridice is a soprano role in Montever...   \n",
       "FURNITURE                             settee A long seat designed for 2 or more peop...   \n",
       "RINGING THE OPENING BELL AT THE NYSE  (Richard) Branson On Oct. 11, 2007 this chairm...   \n",
       "HALLS OF FAME                         the Bee Gees In 1997 their career was \"Stayin'...   \n",
       "\n",
       "                                     question_difficulty  \n",
       "category                                                  \n",
       "WHAT'S WRONG WITH YOU?                           average  \n",
       "GEOMETRY                                            hard  \n",
       "BIBLE PEOPLE MAGAZINE                               easy  \n",
       "ODE TO ENGLAND                                   average  \n",
       "\\\"C\" IN SCIENCE                                     hard  \n",
       "...                                                  ...  \n",
       "BY THE SEASHORE                                     easy  \n",
       "OPERA                                            average  \n",
       "FURNITURE                                           easy  \n",
       "RINGING THE OPENING BELL AT THE NYSE                easy  \n",
       "HALLS OF FAME                                       easy  \n",
       "\n",
       "[13936 rows x 10 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_episode_sub['question_and_answer']\n",
    "regular_episode_reindexed = regular_episode_sub.set_index('category')\n",
    "regular_episode_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "\n",
    "tfidf = vectorizer.fit_transform(regular_episode_reindexed['question_and_answer'])\n",
    "# tfidf.toarray().shape\n",
    "# vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=123)\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "W = nmf.transform(tfidf)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['latent_topic_{}'.format(i) for i in range(n_topics)]\n",
    "# idx= regular_episode_sub['category'] --> change this \n",
    "idx =regular_episode_reindexed.index\n",
    "col = vectorizer.vocabulary_.keys()\n",
    "\n",
    "W = pd.DataFrame(W, index = idx, columns = topics)\n",
    "H = pd.DataFrame(H, index = topics, columns = col)\n",
    "\n",
    "W,H = (np.around(x, 2) for x in (W, H))\n",
    "\n",
    "# print(W.head(30), '\\n\\n', H.head(n_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "city new capital york largest world home founded war french famous west st south old north american born said island\n",
      "\n",
      "Topic #1:\n",
      "man said george time big old born woman book president founded tv song term french author people got life james\n",
      "\n",
      "Topic #2:\n",
      "state new capital island people york west national north largest south river 000 ll lake home set white term year\n",
      "\n",
      "Topic #3:\n",
      "country king south world war largest island great american people capital west hit national years day president north century sea\n",
      "\n",
      "Topic #4:\n",
      "named american president island company founded river born south century red family national north french used year greek henry little\n",
      "\n",
      "Topic #5:\n",
      "like just said wrote hit old says big don author make novel means tv life james 10 letter george seen\n",
      "\n",
      "Topic #6:\n",
      "called used new great group british book william long famous life king meaning century river work don island latin year\n",
      "\n",
      "Topic #7:\n",
      "type used day seen make work war known time white star black letter river red term queen sea long way\n",
      "\n",
      "Topic #8:\n",
      "word used letter meaning term means french latin old greek title said make group just people year red time little\n",
      "\n",
      "Topic #9:\n",
      "john known film title new king novel played president years world war james wrote said character won born book got\n",
      "\n",
      "reconstruction error: 86.70206055462053\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "print (f'reconstruction error: {nmf.reconstruction_err_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nm_factorize(df, col, n_features, n_topics, n_top_words):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "        col ([type]): [description]\n",
    "        n_features ([type]): [description]\n",
    "        n_topics ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\" \n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform(df[col])\n",
    "\n",
    "    nmf = NMF(n_components=n_topics, random_state=123)\n",
    "    nmf.fit(tfidf)\n",
    "\n",
    "    W = nmf.transform(tfidf)\n",
    "    H = nmf.components_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    topics = ['latent_topic_{}'.format(i) for i in range(n_topics)]\n",
    "    idx = df.index\n",
    "    col = vectorizer.vocabulary_.keys()\n",
    "\n",
    "    W = pd.DataFrame(W, index = idx, columns = topics)\n",
    "    H = pd.DataFrame(H, index = topics, columns = col)\n",
    "\n",
    "    W,H = (np.around(x, 2) for x in (W, H))\n",
    "\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "    print (f'RECONSTRUCTION ERROR: {nmf.reconstruction_err_}')\n",
    "#     print ()\n",
    "#     print (W.head(30), '\\n\\n', H.head(n_topics))\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "city capital largest home known world river st 000 famous york born south french north west people national years war\n",
      "\n",
      "Topic #1:\n",
      "man john film said title wrote played king president novel years george book born known james old time won play\n",
      "\n",
      "Topic #2:\n",
      "country king world south american war president island known years largest capital national people born north great 000 west home\n",
      "\n",
      "Topic #3:\n",
      "called book great american people novel john french group little king wrote known white world war life century game sea\n",
      "\n",
      "Topic #4:\n",
      "state capital south north island river home largest national west president known house 000 years war george born 1st said\n",
      "\n",
      "Topic #5:\n",
      "type known letter seen make white red water comes long latin house black work french says greek don 000 book\n",
      "\n",
      "Topic #6:\n",
      "named american french island john king century president british river company year woman known james war born character capital national\n",
      "\n",
      "Topic #7:\n",
      "like said just term wrote means don title song film group says love hit time make known home novel big\n",
      "\n",
      "Topic #8:\n",
      "used word term letter title meaning means french film make known latin comes old greek said time novel war song\n",
      "\n",
      "Topic #9:\n",
      "new york world john film year title island old years day book wrote novel won president played war company born\n",
      "\n",
      "RECONSTRUCTION ERROR: 388.15465072174004\n"
     ]
    }
   ],
   "source": [
    "regular_episodes_reindexed = regular_episodes.set_index('category')\n",
    "n_samples = 2000\n",
    "n_features = 100\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "nm_factorize(regular_episodes_reindexed, 'question_and_answer', n_features, n_topics, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing K:\n",
    "- Plot the reconstruction error for different values of k (elbow plot).\n",
    "- Look at the cosine similarity of items within topics (should be similar) and\n",
    "between topics (should be dissimilar).. If the score is 1: Same orientation, score 0: less similar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-f777841ec13e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# fit transform the feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlda_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# display the lda_output and its shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0midx_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                         self._em_step(X[idx_slice, :], total_samples=n_samples,\n\u001b[0;32m--> 576\u001b[0;31m                                       batch_update=False, parallel=parallel)\n\u001b[0m\u001b[1;32m    577\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# E-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         _, suff_stats = self._e_step(X, cal_sstats=True, random_init=True,\n\u001b[0;32m--> 448\u001b[0;31m                                      parallel=parallel)\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# M-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    399\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_change_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                               random_state)\n\u001b[0;32m--> 401\u001b[0;31m             for idx_slice in gen_even_slices(X.shape[0], n_jobs))\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# merge result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA model\n",
    "count_vectorizer = CountVectorizer(min_df=10, max_df=0.95, ngram_range=(1,1), stop_words='english')\n",
    "feature_matrix = count_vectorizer.fit_transform(regular_episodes_sub['question_and_answer'])\n",
    "    \n",
    "lda_model = LatentDirichletAllocation(n_components=2, max_iter=100, learning_method='online', random_state=43,\n",
    "                                     batch_size=128, evaluate_every=-1, n_jobs=-1)\n",
    "\n",
    "# fit transform the feature matrix\n",
    "lda_output = lda_model.fit_transform(feature_matrix)\n",
    "\n",
    "# display the lda_output and its shape\n",
    "lda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the case study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_indices(df, col_name):\n",
    "    words = df[col_name].values\n",
    "    count_vect = CountVectorizer(lowercase=True, tokenizer=None, stop_words='english',\n",
    "                             analyzer='word', max_df=1.0, min_df=1,\n",
    "                             max_features=None)\n",
    "    # count_vect = CountVectorizer(ngram_range = (1, 2), use_tfidf=True, lowercase=True, \n",
    "    #                             use_stemmer=False, tokenizer=None, stop_words='english',  \n",
    "    #                             max_features=None)\n",
    "    count_vect.fit(words)\n",
    "    count_vect.transform(words)\n",
    "    return count_vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "    return hand_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(df, col, num):\n",
    "    '''\n",
    "    col_name (str): input the column name we want to get the latent topics of \n",
    "    num (int): number of topics we want to get\n",
    "    '''\n",
    "    words = df[col].values\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', strip_accents = 'ascii', ngram_range=(1, 2), \n",
    "                                 lowercase = True, preprocessor = clean_columns(df, col))\n",
    "    vectorizer.fit_transform(words)\n",
    "    vectorizer.vocabulary_\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    vocabulary = np.array(vocabulary)\n",
    "    \n",
    "\n",
    "    nmf_model = NMF(n_components=num, max_iter=100, random_state=12345, alpha=0.0)\n",
    "    W = nmf_model.fit_transform(words)\n",
    "    H = nmf_model.components_\n",
    "    print('reconstruction error:', nmf_model.reconstruction_err_)\n",
    "\n",
    "    return hand_label_topics(H, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SVD or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show(s[:10])\n",
    "\n",
    "num_top_words = 8\n",
    "def show_topics(a):\n",
    "    top_words = lambda t : [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "#show_topics(v[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT STEPS 2\n",
    "- look at the `clustering` assignments and the pdfs\n",
    "- kmean score\n",
    "- silhouette score\n",
    "- try MiniBatchKMean \n",
    "- heirarchical clustering \n",
    "- change the above work so that instead of index, it's printing the category\n",
    "\n",
    "<p>\n",
    "    \n",
    "- output of vectorizer is the array of the bag of words\n",
    "- vocabulary attribute gives word mapped to the index \n",
    "\n",
    "#### Soft Cluster\n",
    "- get a TFIDF matrix\n",
    "- pass the TFIDF as a feature to the NMF \n",
    "- when I print whats in a topic, I want to print the 'category' of the question clas associated with the loadings on that 'category'\n",
    "- identify some outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT STEPS 3\n",
    "- Read through the naive bayes notes, documentation, asssignments\n",
    "- write a Naive Bayes clasifier to classify easy, hard, average questions\n",
    "- cross validate / score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topics to keep track of\n",
    "\n",
    "- clustering (k-means, heirarchical)\n",
    "- pca \n",
    "- svd\n",
    "- nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Naive Bayes\n",
    "- Construct a naive bayes on the words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
