{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pyLDAvis.sklearn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_episodes = pd.read_csv(\"../data/jeopardy_regular_episodes.csv\")\n",
    "sample = regular_episodes.sample(frac=0.01)\n",
    "text = regular_episodes['Question and Answer'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278730, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_episodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stopwords(filepath='../src/stopwords.txt'):\n",
    "    \"\"\"\n",
    "    read in a list of stopwords from a .txt file\n",
    "    and extend the nltk stopwords by this list.\n",
    "    Return a list of stopwords created from nltk\n",
    "    and the .txt file\n",
    "    \"\"\"\n",
    "    sw = open(filepath, \"r\")\n",
    "    my_stopwords = sw.read()\n",
    "    my_stopwords = my_stopwords.split(\", \")\n",
    "    sw.close()\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.extend(my_stopwords)\n",
    "    return all_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_hypens(text):\n",
    "    return re.sub(r'(\\w+)-(\\w+)-?(\\w)?', r'\\1 \\2 \\3', text)\n",
    "\n",
    "\n",
    "# tokenize text\n",
    "def tokenize_text(text):\n",
    "    TOKEN_PATTERN = r'\\s+'\n",
    "    regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=True)\n",
    "    word_tokens = regex_wt.tokenize(text)\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def convert_to_lowercase(tokens):\n",
    "    return [token.lower() for token in tokens if token.isalpha()]\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens, stop_words):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    stopword_list += stop_words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def get_lemma(tokens):\n",
    "    lemmas = []\n",
    "    for word in tokens:\n",
    "        lemma = wn.morphy(word)\n",
    "        if lemma is None:\n",
    "            lemmas.append(word)\n",
    "        else:\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def remove_short_tokens(tokens):\n",
    "    return [token for token in tokens if len(token) > 3]\n",
    "\n",
    "\n",
    "def keep_only_words_in_wordnet(tokens):\n",
    "    return [token for token in tokens if wn.synsets(token)]\n",
    "\n",
    "\n",
    "def apply_lemmatize(tokens, wnl=WordNetLemmatizer()):\n",
    "    return [wnl.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "def clean_text_clues(texts):\n",
    "    clean_clues = []\n",
    "    for clue in texts:\n",
    "        clue = remove_hypens(clue)\n",
    "        clue_i = tokenize_text(clue)\n",
    "        clue_i = remove_characters_after_tokenization(clue_i)\n",
    "        clue_i = convert_to_lowercase(clue_i)\n",
    "        clue_i = remove_stopwords(clue_i, stop_words)\n",
    "        clue_i = get_lemma(clue_i)\n",
    "        clue_i = remove_short_tokens(clue_i)\n",
    "        clue_i = keep_only_words_in_wordnet(clue_i)\n",
    "        clue_i = apply_lemmatize(clue_i)\n",
    "        clean_clues.append(clue_i)\n",
    "    return clean_clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = make_stopwords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_clues = clean_text_clues(text)\n",
    "clean_clues_text = [' '.join(item) for item in clean_clues]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=10, max_df=0.95, ngram_range=(1,2), stop_words=stop_words)\n",
    "feature_matrix = count_vectorizer.fit_transform(clean_clues_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter=10, learning_method='online', random_state=43,\n",
    "                                     batch_size=128, evaluate_every=-1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_output = lda_model.fit_transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lda_output) #output\n",
    "display(lda_output.shape) #shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.score(feature_matrix) #log-likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.perplexity(feature_matrix) #perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE TO THE GRIDSEARCH PARAMETERS\n",
    "\n",
    "search_params = {'n_components': [10, 12, 13, 15, 20, 25], 'learning_decay': [.5, .7, .9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation()\n",
    "model = GridSearchCV(lda, search_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model's params: \", model.best_params_)\n",
    "print(\"Best log likelihood score: \", model.best_score_)\n",
    "print(\"Model perplexity: \", best_lda_model.perplexity(feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(model.cv_results_)\n",
    "df_cv_results.to_csv(\"../data/LDAGridSearchResults.csv\", header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"param_n_components\", y=\"mean_test_score\", hue=\"param_learning_decay\", data=df_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this information to tell em the best hyperparameters for lda\n",
    "best_lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these hyperparameters to fit the best_lda_model info\n",
    "LatentDirichletAllocation(learning_decay=.7,\n",
    "             learning_method=\"batch\", learning_offset=10.0,\n",
    "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    "             n_components=10, n_jobs=None, perp_tol=0.1,\n",
    "             random_state=None, topic_word_prior=None,\n",
    "             total_samples=1000000.0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output = best_lda_model.transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "topicnames = ['Topic_' + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = ['Doc_' + str(i) for i in range(len(text))]\n",
    "\n",
    "# create a dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output,2), columns=topicnames, index=docnames)\n",
    "\n",
    "df_document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dominant topic\n",
    "df_document_topic['dominant_topic'] = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_document_topic.dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mds = tsne\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, feature_matrix, count_vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDS = PCoA\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, feature_matrix, count_vectorizer, mds='PCoA')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components_ contains the word to topic matrix\n",
    "best_lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic - Keyword matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# assign column and index\n",
    "df_topic_keywords.columns = count_vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "\n",
    "# check the head\n",
    "df_topic_keywords.iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=count_vectorizer, lda_model=best_lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = show_topics(count_vectorizer, best_lda_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
