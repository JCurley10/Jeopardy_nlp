# NMF MODEL FUNCTIONS
# REPLACING THE DF, COL PARAMETERS WITH CLUE = A LIST

import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from wordcloud import WordCloud
from gensim import models

from sklearn.decomposition import NMF
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity

from text_cleaner import clean_text_clues, convert_col_to_list, make_stopwords


class NMF_Model():

    def __init__(self, text, factorizer, vectorizer, n_topics):
        self.text = text
        self.factorizer = factorizer
        self.vectorizer = vectorizer
        self.n_topics = n_topics

    def vec_to_mat(self):
        tfidf_vectorizer = self.vectorizer
        self.tfidf = tfidf_vectorizer.fit_transform(self.text)

        nmf = self.factorizer
        nmf.fit_transform(self.tfidf)  # W matrix

    def get_names_weights(self):
        """
        """
        #Put these following functions to vec_to_mat 
        # tfidf_vectorizer = self.vectorizer
        # self.tfidf = tfidf_vectorizer.fit_transform(self.text)
        # nmf = self.factorizer
        # nmf.fit_transform(tfidf)  # W matrix
        tfidf_vectorizer = self.vectorizer
        self.feature_names = tfidf_vectorizer.get_feature_names()
        return self.feature_names  # Feature names

    def get_factorization_matrix(self):
        return self.factorizer.components_  # H matrix

    def get_reconstruction_error(self):
        return self.factorizer.reconstruction_err_

    # def find_best_k(self, testing_topics):
    #     '''
    #     plot the number of topics vs the reconstruction error
    #     to look at the elbow and decide on which number
    #     of topics is best

    #     Args:
    #         df (Pandas DataFrame): DataFrame with the top categories
    #         col (str): column name to get the clusters
    #         vectorizer (type TfidfVectorizer() vectorizer): method to vectorize the text
    #         testing_topics (int): total number of topics to get clusters of
    #         nmf (type sklearn NMF decomposer): initialized NMF instance
    #     Returns:
    #         None:
    #     '''
    #     errs = []
    #     for k in range(testing_topics):
    #         self.n_topics
    #         errs.append(self.factorizer.reconstruction_err_)
    #         plt.plot(range(testing_topics), errs)
    #     plt.xlabel('k')
    #     plt.ylabel('Reconstruction Error')
    #     # plt.show()

    def cluster_words(self, n_top_words):
        """
        """

        self.words = []
        self.topic_indices = []
        for topic_idx, topic in enumerate(self.get_factorization_matrix()):
            self.words.append(list(self.get_names_weights()[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]))
            self.topic_indices.append(topic_idx)

        return self.words


    def make_words_and_weights_lists(self):
        """
        get a list of arrays that have the words per cluster, and their
        associated weights as a list of len 2.
        weights = nmf.components_ = nmf.facorization_matrix
        """
        feature_names_array = np.array(self.get_names_weights())
        sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(self.get_factorization_matrix()))])
        sorted_weights = np.array([list(wt[index]) for wt, index in zip(self.get_factorization_matrix(), sorted_indices)])
        sorted_terms = np.array([list(feature_names_array[row]) for row in sorted_indices])

        self.words_weights_lst = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]
        return self.words_weights_lst


    def make_words_and_weights_dict(self, nth_topic, n_top_words):
        """
        make a dictionary where the keys are the words or feature_names
        and the value are the associate weights. This will be used
        for graphing a wordcloud generated by word frequencies
        and the weights will be the frequencies

        Args:
            topics (list of lists  of strings): the feature_names
                (or words) with their weights as strings
            n_topics (int): total number of topics to get clusters of
            n_top_words (int): number of words per cluster
            sorted_weights: Numpy Array of weights, in descending order,
                associated with each vector associated with its corresponding
                list index in feature_names

        Returns:
            Dictionary: the feature_names or words are the keys and the
            values are the associated weights
        """
        top_topics = []
        for topic in self.make_words_and_weights_lists():
            top_topics.append(topic[:10])

        top_n_topics = []
        i = nth_topic
        for j in range(n_top_words):
            top_n_topics.append(top_topics[i][j][0])
            top_n_topics.append(np.sqrt(float(top_topics[i][j][1])))

        self.words_weights_dictionary = dict(zip(top_n_topics[::2], top_n_topics[1::2]))
        return self.words_weights_dictionary

    def make_word_cloud(self, nth_topic, n_top_words, color='plasma', save=False):
        """
        Make a wordcloud of the feature_names (words)
        from a single cluster or topic

        Args:
            dictionary ([type]): [description]
            color (str): matplotlib colormap
            n (int): The topic number to make a wordcloud of
            save (bool, optional): If True, saves figure to filepath
                If False, shows figure. Defaults to False.
        Returns:
            None. Generates a wordcloud, and either saves or shows it
        """
        wordcloud = WordCloud(width=800, height=800,
                              relative_scaling=.5, normalize_plurals=True,
                              background_color=None, mode='RGBA',
                              colormap=color, collocations=False,
                              min_font_size=10)

        wordcloud.generate_from_frequencies(self.make_words_and_weights_dict(nth_topic, n_top_words))

        # plot the WordCloud image
        plt.figure(figsize=(8, 8), facecolor=None)
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.tight_layout(pad=0)

        if save:
            plt.savefig(f'../images/{nth_topic}_topic_model_Wordcloud.png')
        else:
            plt.show()

    def show_word_clouds(self, n_top_words, color='plasma', save=False):
        """
        Show or save the wordclouds for all topics

        Args:
            n_topics (int): total number of topics to get clusters of
            topics: list of lists  of strings
                the feature_names (or words) with their weights as strings
            n_top_words (int): number of words per cluster
            color (str, optional): matplotlib color. Defaults to 'plasma'.
            save (bool, optional): If True, saves figure to filepath
                If False, shows figure. Defaults to False.
        Returns:
            None. saves or shows the wordclouds
        """
        for nth_topic in range(self.n_topics):
            self.make_word_cloud(nth_topic, n_top_words, color=color, save=save)


if __name__ == "__main__":
    regular_episodes = pd.read_csv("../data/jeopardy_regular_episodes.csv")
    regular_episodes_sub = regular_episodes.sample(frac=.02)

    # To look at W and H with respect to the original J-categories
    regular_episode_sub_reindexed = regular_episodes_sub.set_index('J-Category')
    regular_episodes_reindexed = regular_episodes.set_index('J-Category')

    rough_clues_sub = convert_col_to_list(regular_episode_sub_reindexed, "Question and Answer")
    rough_clues = convert_col_to_list(regular_episodes_reindexed, 'Question and Answer')
    clues = clean_text_clues(rough_clues)

    stop_words = make_stopwords("stopwords.txt")
    tot_features = 1000
    n_topics = 13

    # Adjust the vectorizer and nmf model hyperparameters
    nmf = NMF(n_components=n_topics, random_state=123,
              alpha=0.1, l1_ratio=0.5, max_iter=1000)

    vectorizer = TfidfVectorizer(
                    ngram_range=(1, 2), strip_accents='ascii',
                    lowercase=True, tokenizer=None,
                    analyzer='word', stop_words=stop_words,
                    max_features=tot_features)

    # instantiate the class NMF_model
    model = NMF_Model(text=clues, factorizer=nmf, vectorizer=vectorizer, n_topics=n_topics)
    model.vec_to_mat()
    model.make_words_and_weights_dict(5, 10)
