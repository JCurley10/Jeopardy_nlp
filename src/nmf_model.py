import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.decomposition import NMF
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
from sklearn.model_selection import GridSearchCV
# Local Imports
from text_cleaner import clean_text_clues, convert_col_to_list, make_stopwords, token_by_lemma


class NMF_Model():
    """
    This class performs vectorization of text
    and non-negative matrix factorization (NMF)
    on corpus of text as lists of tokens.
    It is initialized with the text, type of facttorizer, 
    type of vectorizer, and number of topics/clusters to consider
    """

    def __init__(self, text, factorizer, vectorizer, n_topics):
        self.text = text
        self.factorizer = factorizer
        self.vectorizer = vectorizer
        self.n_topics = n_topics

    def vec_to_mat(self):
        """
        Fits and transforms the tfidf vectorizer to the text
        Then fits and transforms the nmf model using the tdidf matrix.
        Must be done before calling other methods in this class.
        """
        tfidf_vectorizer = self.vectorizer
        self.tfidf = tfidf_vectorizer.fit_transform(self.text)

        nmf = self.factorizer
        nmf.fit_transform(self.tfidf)  # W matrix

    def get_names_weights(self):
        """
        Get a list of tokens (words as strings) that are
        considered in the model
        """
        tfidf_vectorizer = self.vectorizer
        self.feature_names = tfidf_vectorizer.get_feature_names()
        return self.feature_names

    def get_factorization_matrix(self):
        """
        Get the factorized matrix, an ndarray
        of shape (n_components, n_features)
        """
        return self.factorizer.components_  # H matrix

    def get_reconstruction_error(self):
        """
        Get the reconstruction error of the factored matrices
        with the associated n_topics
        """
        return self.factorizer.reconstruction_err_

    def cluster_words(self, n_top_words):
        """
        Generates clusters of words
        -------
        Args:
            n_top_words (int): number of words to see in each cluster
        Returns:
            self.words (list of list of strings): each string is a token in
                a cluster, and each inner list is a cluster of tokens
        """
        self.words = []
        self.topic_indices = []
        for topic_idx, topic in enumerate(self.get_factorization_matrix()):
            self.words.append(list(self.get_names_weights()[i]
                                   for i in topic.argsort()[:-n_top_words - 1:-1]))
            self.topic_indices.append(topic_idx)

        return self.words

    def make_words_and_weights_lists(self):
        """
        Get a list of arrays that have the words per cluster, and their
        associated weights as a list of length 2
        Note: = sklearn's nmf.components_ = self.get_facorization_matrix()
        """
        feature_names_array = np.array(self.get_names_weights())
        sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(self.get_factorization_matrix()))])
        sorted_weights = np.array([list(wt[index]) for wt, index in zip(self.get_factorization_matrix(), sorted_indices)])
        sorted_terms = np.array([list(feature_names_array[row]) for row in sorted_indices])

        self.words_weights_lst = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]
        return self.words_weights_lst

    def make_words_and_weights_dict(self, nth_topic, n_top_words):
        """
        make a dictionary where the keys are the words or feature_names
        and the value are the associate weights. This will be used
        for graphing a wordcloud generated by word frequencies
        and the weights will be the frequencies
        --------
        Args:
            topics (list of lists  of strings): the feature_names
                (or words) with their weights as strings
            n_topics (int): total number of topics to get clusters of
            n_top_words (int): number of words per cluster
            sorted_weights: Numpy Array of weights, in descending order,
                associated with each vector associated with its corresponding
                list index in feature_names
        Returns:
            Dictionary: the feature_names or words are the keys and the
            values are the associated weights
        """
        top_topics = []
        for topic in self.make_words_and_weights_lists():
            top_topics.append(topic[:n_top_words])

        top_n_topics = []
        i = nth_topic
        for j in range(n_top_words):
            top_n_topics.append(top_topics[i][j][0])
            top_n_topics.append(np.sqrt(float(top_topics[i][j][1])))

        self.words_weights_dictionary = dict(zip(top_n_topics[::2], top_n_topics[1::2]))
        return self.words_weights_dictionary

    def make_word_cloud(self, nth_topic, n_top_words, color='magma', save=False):
        """
        Make a wordcloud of the feature_names (words)
        from a single cluster or topic
        --------
        Args:
            nth_topic (int): The topic number to show a wordcloud of.
                Must be between 0 and n_topics-1.
            n_top_words (int): The number of top words to show in the cloud.
            color (str, optional): matplotlib color. Defaults to 'magma'.
            save (bool, optional): whether to save or show the figure.
                save=True saves the figure, save=False shows the figure.
                Defaults to False.
        Returns:
            None. Generates a wordcloud, and either saves or shows it
        """
        wordcloud = WordCloud(width=800, height=800,
                              relative_scaling=.5, normalize_plurals=True,
                              background_color=None, mode='RGBA',
                              colormap=color, collocations=False,
                              min_font_size=10)

        wordcloud.generate_from_frequencies(self.make_words_and_weights_dict(nth_topic, n_top_words))

        # plot the WordCloud image
        plt.figure(figsize=(8, 8), facecolor=None)
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.tight_layout(pad=0)

        if save:
            plt.savefig(f'../images/_ppt_{nth_topic}_topic_model_Wordcloud.png')
        else:
            plt.show()

    def show_word_clouds(self, n_top_words, color='magma', save=False):
        """
        Show or save the wordclouds for all topics
        -------
        Args:
            n_topics (int): total number of topics to get clusters of
            topics: list of lists  of strings
                the feature_names (or words) with their weights as strings
            n_top_words (int): number of words per cluster
            color (str, optional): matplotlib color. Defaults to 'plasma'.
            save (bool, optional): If True, saves figure to filepath
                If False, shows figure. Defaults to False.
        Returns:
            None. saves or shows the wordclouds
        """
        for nth_topic in range(self.n_topics):
            self.make_word_cloud(nth_topic, n_top_words, color=color,
                                 save=save)


def find_best_k(vectorizer, lower_k, upper_k, text):
    """
    Find the reconstruction errors for the number of clusters given the same
    model, but changing the number of clusters
    -------
    Args:
        vectorizer (sklearn's TfidfVectorizer): instantiated with the
            appropriate hyperparameters used to build the model settled on.
        lower_k (int): lower bound of the number of clusters to check
            the reconstruction error of
        upper_k (int): upper bound of the number of clusters to check
            the reconstruction error of, noninclusive
    Returns:
        list of floats: list of reconstruction errors of
            length lower_k-upper_k-1
    """
    recon_errs = []
    for n_topic in range(lower_k, upper_k):
        nmf = NMF(n_components=n_topic, init='nndsvd', random_state=43,
                  max_iter=1000)
        model = NMF_Model(text=text, factorizer=nmf, vectorizer=vectorizer,
                          n_topics=n_topic)
        model.vec_to_mat()
        recon_errs.append(model.get_reconstruction_error())
    return recon_errs


def plot_ks(vectorizer, lower_k, upper_k):
    """
    Plot a line graph that shows the number of clusters along x-axis
    and the reconstruction error along the y axis
    -------
    Args:
        vectorizer (sklearn's TfidfVectorizer): instantiated with the
            appropriate hyperparameters used to build the model settled on.
        lower_k (int): lower bound of the number of clusters to check
            the reconstruction error of
        upper_k (int): upper bound of the number of clusters to check
            the reconstruction error of, noninclusive
    Returns:
        None. Saves the image under the images directory as a .png 
    """
    fig, ax = plt.subplots(figsize=(8, 6), dpi=150)
    x = range(lower_k, upper_k)
    y = find_best_k(vectorizer, lower_k, upper_k)
    ax.plot(x, y, color='darkmagenta')
    ax.set_ylabel("Reconstruction Error", fontsize=14)
    ax.set_xlabel("Number of Clusters", fontsize=14)
    ax.set_title("Number of Clusters vs. Reconstruction Error", fontsize=16)
    plt.tight_layout()
    plt.savefig("../images/reconerr_vs_k.png")


if __name__ == "__main__":
    # Read in the jeopardy_regular_episodes.csv file
    regular_episodes = pd.read_csv("../data/regular_episodes.csv")

    # To look at W and H with respect to the original J-categories
    regular_episodes_reindexed = regular_episodes.set_index('J-Category')

    # convert a column of text to a list to run through the cleaning pipeline
    rough_clues = convert_col_to_list(regular_episodes_reindexed,
                                      'Question and Answer')
    # Clean the text from  rough_clues
    clues = clean_text_clues(rough_clues)

    # Instantiate hyperparameters
    stop_words = make_stopwords("stopwords.txt")
    tot_features = 1000
    n_topics = 13
    n_top_words = 10

    # Adjust the vectorizer and nmf model hyperparameters
    nmf = NMF(n_components=n_topics, init='nndsvd', random_state=43,
              max_iter=1000)

    vectorizer = TfidfVectorizer(
                    ngram_range=(1, 2), strip_accents='ascii',
                    lowercase=True, tokenizer=None,
                    analyzer='word', stop_words=stop_words,
                    max_features=1000)

    # Instantiate the class NMF_model
    model = NMF_Model(text=clues, factorizer=nmf, vectorizer=vectorizer,
                      n_topics=n_topics)
